# Epstein GraphRAG - vLLM Worker (FASTEST + CHEAPEST)
# 
# vLLM provides 5-10x higher throughput than Ollama via continuous batching.
# Model: Qwen2-VL-7B (~7GB, 13GB VRAM max) - fits on RTX 3060/T4
#
# COST ESTIMATE for 850K PDFs:
#   - With vLLM: ~5 sec/page average (vs 30 sec with Ollama)
#   - Total GPU-hours: ~3,750 hrs (vs 22,500 with Ollama)
#   - At $0.05/hr (RTX 3060): ~$190 total
#   - At $0.02/hr (T4): ~$75 total

FROM nvidia/cuda:12.4.1-devel-ubuntu22.04

ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONUNBUFFERED=1

# System dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    python3.11 \
    python3.11-venv \
    python3.11-dev \
    python3-pip \
    poppler-utils \
    libgl1-mesa-glx \
    libglib2.0-0 \
    curl \
    git \
    && rm -rf /var/lib/apt/lists/*

# Make python3.11 default
RUN update-alternatives --install /usr/bin/python python /usr/bin/python3.11 1 \
    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.11 1

# Install uv
RUN curl -LsSf https://astral.sh/uv/install.sh | sh
ENV PATH="/root/.local/bin:$PATH"

WORKDIR /app

# Install vLLM first (takes a while, cached layer)
RUN uv venv /app/.venv && \
    . /app/.venv/bin/activate && \
    pip install vllm>=0.6.0

# Copy and install project
COPY pyproject.toml uv.lock ./
COPY src/ ./src/

RUN . /app/.venv/bin/activate && uv sync --frozen

ENV PATH="/app/.venv/bin:$PATH"
ENV VIRTUAL_ENV="/app/.venv"

# Pre-download the model (makes image ~15GB but instant startup)
# Uses HuggingFace cache
ENV HF_HOME=/app/.cache/huggingface
RUN . /app/.venv/bin/activate && \
    python -c "from huggingface_hub import snapshot_download; snapshot_download('Qwen/Qwen2-VL-7B-Instruct')"

# Create data directories
RUN mkdir -p /data/processed /data/extracted

# Copy entrypoint
COPY deploy/vllm-entrypoint.sh /entrypoint.sh
RUN chmod +x /entrypoint.sh

# vLLM API port
EXPOSE 8000

ENTRYPOINT ["/entrypoint.sh"]
